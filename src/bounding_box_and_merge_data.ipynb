{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.config\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda91489",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = Path(os.path.dirname(os.path.abspath(''))).resolve() / \"data\"\n",
    "DIR_SOURCE = DIR_DATA / \"raw\"\n",
    "DIR_OUTPUT = DIR_DATA / \"processed\"\n",
    "DIR_OUTPUT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62c25ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw directory exists: True\n",
      "Files in raw data directory: 40906\n",
      "Raw data size: 511.96 GB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Raw directory exists: {DIR_SOURCE.is_dir()}\")\n",
    "    print(f\"Files in raw data directory: {sum([len(files) for _, _, files in os.walk(DIR_SOURCE)])}\")\n",
    "    print(f\"Raw data size: {(sum(\n",
    "        os.path.getsize(\n",
    "            os.path.join(dirpath,filename)\n",
    "            ) \n",
    "            for dirpath, dirnames, filenames in os.walk(DIR_SOURCE) \n",
    "            for filename in filenames \n",
    "        )\n",
    "    ) / (1000 ** 3):.2f} GB\")\n",
    "except Exception as e:\n",
    "    print(f\"Raw data directory: error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6021238",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_VARS = {\n",
    "    \"ECCO_L4_MIXED_LAYER_DEPTH_05DEG_DAILY_V4R4\" : ['MXLDEPTH'],\n",
    "    \"ECCO_L4_SSH_05DEG_DAILY_V4R4\" : ['SSH', 'SSHIBC'],\n",
    "    \"OSCAR_L4_OC_FINAL_V2.0\" : ['u', 'v'],\n",
    "    \"OSTIA-UKMO-L4-GLOB-REP-v2.0\" : ['analysed_sst'],\n",
    "    \"ECCO_L4_FRESH_FLUX_05DEG_DAILY_V4R4\" : ['EXFempmr', 'SFLUX'],\n",
    "    \"ECCO_L4_DENS_STRAT_PRESS_05DEG_DAILY_V4R4\" : ['PHIHYD', 'RHOAnoma'],\n",
    "    \"AQUA_MODIS_L3M_DAILY_4KM_CHLCONC\": ['chlor_a'],\n",
    "    \"AQUA_MODIS_L3M_DAILY_4KM_PIC\": ['pic'],\n",
    "    \"AQUA_MODIS_L3M_DAILY_4KM_POC\": ['poc'],\n",
    "}\n",
    "\n",
    "DATASETS_SHORTHAND = {\n",
    "    \"ECCO_L4_MIXED_LAYER_DEPTH_05DEG_DAILY_V4R4\" : \"MIXED_LAYER_DEPTH\",\n",
    "    \"ECCO_L4_SSH_05DEG_DAILY_V4R4\" : \"SSH_DAILY\",\n",
    "    \"OSCAR_L4_OC_FINAL_V2.0\" : \"CURRENTS\",\n",
    "    \"OSTIA-UKMO-L4-GLOB-REP-v2.0\" : \"SST\",\n",
    "    \"ECCO_L4_FRESH_FLUX_05DEG_DAILY_V4R4\" : \"FRESHWATER_FLUX\",\n",
    "    \"ECCO_L4_DENS_STRAT_PRESS_05DEG_DAILY_V4R4\" : \"DENSITY\",\n",
    "    \"AQUA_MODIS_L3M_DAILY_4KM_CHLCONC\": \"CHLOROPHYLL\",\n",
    "    \"AQUA_MODIS_L3M_DAILY_4KM_PIC\": \"PIC\",\n",
    "    \"AQUA_MODIS_L3M_DAILY_4KM_POC\": \"POC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f41175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(x, lon_bnds, lat_bnds, dataset):\n",
    "\n",
    "    x = x[DATASETS_VARS[dataset]]\n",
    "    try:\n",
    "        x = x.rename({\n",
    "            \"Time\": \"time\",\n",
    "            \"Longitude\": \"lon\",\n",
    "            \"Latitude\": \"lat\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    try:\n",
    "        x = x.rename({\n",
    "            \"longitude\": \"lon\",\n",
    "            \"latitude\": \"lat\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    x = x.dropna(dim='time', how='all', subset=DATASETS_VARS[dataset])\n",
    "\n",
    "    variables = dict().fromkeys(DATASETS_VARS[dataset], np.float32)\n",
    "    x = x.astype(variables)\n",
    "\n",
    "    if pd.Series(x['lat'].values).is_monotonic_increasing:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[0], lat_bnds[1]))\n",
    "    else:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[1], lat_bnds[0]))\n",
    "    \n",
    "\n",
    "def _preprocess_OSCAR(x, lon_bnds, lat_bnds):\n",
    "\n",
    "    x = x[DATASETS_VARS[\"OSCAR_L4_OC_FINAL_V2.0\"]]\n",
    "    x = x.swap_dims({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "    x = x.dropna(dim='time', how='all', subset=DATASETS_VARS[\"OSCAR_L4_OC_FINAL_V2.0\"])\n",
    "\n",
    "    x['lon'] = (x['lon'] + 180) % 360 - 180\n",
    "\n",
    "    variables = dict().fromkeys(DATASETS_VARS[\"OSCAR_L4_OC_FINAL_V2.0\"], np.float32)\n",
    "    x = x.astype(variables)\n",
    "    \n",
    "    if pd.Series(x['lat'].values).is_monotonic_increasing:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[0], lat_bnds[1]))\n",
    "    else:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[1], lat_bnds[0]))\n",
    "\n",
    "def _preprocess_DENSITY(x, lon_bnds, lat_bnds):\n",
    "\n",
    "    x = x[DATASETS_VARS[\"ECCO_L4_DENS_STRAT_PRESS_05DEG_DAILY_V4R4\"]]\n",
    "    try:\n",
    "        x = x.rename({\n",
    "            \"longitude\": \"lon\",\n",
    "            \"latitude\": \"lat\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    x = x.dropna(dim='time', how='all', subset=DATASETS_VARS[\"ECCO_L4_DENS_STRAT_PRESS_05DEG_DAILY_V4R4\"])\n",
    "\n",
    "    variables = dict().fromkeys(DATASETS_VARS[\"ECCO_L4_DENS_STRAT_PRESS_05DEG_DAILY_V4R4\"], np.float32)\n",
    "    x = x.astype(variables)\n",
    "    x = x.sel(Z=slice(-5, -100))\n",
    "    x = x.mean(dim='Z')\n",
    "\n",
    "    if pd.Series(x['lat'].values).is_monotonic_increasing:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[0], lat_bnds[1]))\n",
    "    else:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[1], lat_bnds[0]))\n",
    "    \n",
    "def _preprocess_MODIS(x, lon_bnds, lat_bnds, dataset):\n",
    "\n",
    "    filename = x.attrs[\"product_name\"]\n",
    "    time = filename[11:19]\n",
    "    time = datetime.strptime(time, \"%Y%m%d\")\n",
    "    time = pd.to_datetime(time)\n",
    "\n",
    "    x = x.assign_coords(time=time)\n",
    "    x = x.expand_dims(dim='time')\n",
    "\n",
    "    x = x[DATASETS_VARS[dataset]]\n",
    "\n",
    "    x = x.dropna(dim='time', how='all', subset=DATASETS_VARS[dataset])\n",
    "\n",
    "    variables = dict().fromkeys(DATASETS_VARS[dataset], np.float32)\n",
    "    x = x.astype(variables)\n",
    "    \n",
    "    if pd.Series(x['lat'].values).is_monotonic_increasing:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[0], lat_bnds[1]))\n",
    "    else:\n",
    "        return x.sel(lon=slice(*lon_bnds), lat=slice(lat_bnds[1], lat_bnds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744226b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:39055' processes=5 threads=10, memory=25.00 GiB>\n",
      "Dataset: AQUA_MODIS_L3M_DAILY_4KM_CHLCONC\n",
      "<xarray.Dataset> Size: 30GB\n",
      "Dimensions:  (time: 4069, lat: 1200, lon: 1560)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 5kB 64.98 64.94 64.9 64.85 ... 15.15 15.1 15.06 15.02\n",
      "  * lon      (lon) float32 6kB -180.0 -179.9 -179.9 ... -115.1 -115.1 -115.0\n",
      "  * time     (time) datetime64[ns] 33kB 2002-07-05 2002-07-06 ... 2013-08-31\n",
      "Data variables:\n",
      "    chlor_a  (time, lat, lon) float32 30GB dask.array<chunksize=(1, 1200, 1560), meta=np.ndarray>\n",
      "Attributes: (12/49)\n",
      "    instrument:                       MODIS\n",
      "    title:                            MODISA Level-3 Standard Mapped Image\n",
      "    project:                          Ocean Biology Processing Group (NASA/GS...\n",
      "    platform:                         Aqua\n",
      "    source:                           satellite observations from MODIS-Aqua\n",
      "    processing_version:               R2022.0\n",
      "    ...                               ...\n",
      "    creator_url:                      https://oceandata.sci.gsfc.nasa.gov\n",
      "    publisher_url:                    https://oceandata.sci.gsfc.nasa.gov\n",
      "    processing_level:                 L3 Mapped\n",
      "    cdm_data_type:                    grid\n",
      "    keywords:                         Earth Science > Oceans > Ocean Chemistr...\n",
      "    keywords_vocabulary:              NASA Global Change Master Directory (GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isekar/.conda/envs/SpaceApps2025/lib/python3.13/site-packages/zarr/api/asynchronous.py:244: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n",
      "/home/isekar/.conda/envs/SpaceApps2025/lib/python3.13/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 10.04 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: AQUA_MODIS_L3M_DAILY_4KM_PIC\n",
      "<xarray.Dataset> Size: 30GB\n",
      "Dimensions:  (time: 4070, lat: 1200, lon: 1560)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 5kB 64.98 64.94 64.9 64.85 ... 15.15 15.1 15.06 15.02\n",
      "  * lon      (lon) float32 6kB -180.0 -179.9 -179.9 ... -115.1 -115.1 -115.0\n",
      "  * time     (time) datetime64[ns] 33kB 2002-07-04 2002-07-05 ... 2013-08-31\n",
      "Data variables:\n",
      "    pic      (time, lat, lon) float32 30GB dask.array<chunksize=(1, 1200, 1560), meta=np.ndarray>\n",
      "Attributes: (12/49)\n",
      "    instrument:                       MODIS\n",
      "    title:                            MODISA Level-3 Standard Mapped Image\n",
      "    project:                          Ocean Biology Processing Group (NASA/GS...\n",
      "    platform:                         Aqua\n",
      "    source:                           satellite observations from MODIS-Aqua\n",
      "    processing_version:               R2022.0\n",
      "    ...                               ...\n",
      "    creator_url:                      https://oceandata.sci.gsfc.nasa.gov\n",
      "    publisher_url:                    https://oceandata.sci.gsfc.nasa.gov\n",
      "    processing_level:                 L3 Mapped\n",
      "    cdm_data_type:                    grid\n",
      "    keywords:                         Earth Science > Oceans > Ocean Chemistr...\n",
      "    keywords_vocabulary:              NASA Global Change Master Directory (GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isekar/.conda/envs/SpaceApps2025/lib/python3.13/site-packages/zarr/api/asynchronous.py:244: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n",
      "/home/isekar/.conda/envs/SpaceApps2025/lib/python3.13/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 10.44 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: AQUA_MODIS_L3M_DAILY_4KM_POC\n",
      "<xarray.Dataset> Size: 30GB\n",
      "Dimensions:  (time: 4070, lat: 1200, lon: 1560)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 5kB 64.98 64.94 64.9 64.85 ... 15.15 15.1 15.06 15.02\n",
      "  * lon      (lon) float32 6kB -180.0 -179.9 -179.9 ... -115.1 -115.1 -115.0\n",
      "  * time     (time) datetime64[ns] 33kB 2002-07-04 2002-07-05 ... 2013-08-31\n",
      "Data variables:\n",
      "    poc      (time, lat, lon) float32 30GB dask.array<chunksize=(1, 1200, 1560), meta=np.ndarray>\n",
      "Attributes: (12/49)\n",
      "    instrument:                       MODIS\n",
      "    title:                            MODISA Level-3 Standard Mapped Image\n",
      "    project:                          Ocean Biology Processing Group (NASA/GS...\n",
      "    platform:                         Aqua\n",
      "    source:                           satellite observations from MODIS-Aqua\n",
      "    processing_version:               R2022.0\n",
      "    ...                               ...\n",
      "    creator_url:                      https://oceandata.sci.gsfc.nasa.gov\n",
      "    publisher_url:                    https://oceandata.sci.gsfc.nasa.gov\n",
      "    processing_level:                 L3 Mapped\n",
      "    cdm_data_type:                    grid\n",
      "    keywords:                         Earth Science > Oceans > Ocean Chemistr...\n",
      "    keywords_vocabulary:              NASA Global Change Master Directory (GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isekar/.conda/envs/SpaceApps2025/lib/python3.13/site-packages/zarr/api/asynchronous.py:244: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n",
      "/home/isekar/.conda/envs/SpaceApps2025/lib/python3.13/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 10.44 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dask.config.set({'array.chunk-size': '1024MiB'})\n",
    "\n",
    "with LocalCluster(n_workers=5, threads_per_worker=2 ,silence_logs=logging.ERROR, memory_limit=\"5GiB\") as cluster:\n",
    "    with Client(cluster) as client:\n",
    "\n",
    "        print(client)\n",
    "\n",
    "        for dataset, vars in DATASETS_VARS.items():\n",
    "\n",
    "            new_dataset_path = DIR_OUTPUT / dataset\n",
    "            new_dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if len(os.listdir(DIR_OUTPUT / dataset)) == 1:\n",
    "                continue\n",
    "\n",
    "            print(f\"Dataset: {dataset}\")\n",
    "            \n",
    "            files = os.path.join(DIR_SOURCE / dataset, \"*.nc\")\n",
    "\n",
    "            lon_bnds, lat_bnds = (-180, -115), (15, 65)\n",
    "\n",
    "            if dataset == \"OSCAR_L4_OC_FINAL_V2.0\":\n",
    "                partial_func = partial(_preprocess_OSCAR, lon_bnds=lon_bnds, lat_bnds=lat_bnds)\n",
    "                ds = xr.open_mfdataset(\n",
    "                    files, \n",
    "                    preprocess=partial_func, \n",
    "                    join='outer', \n",
    "                    combine_attrs='drop_conflicts',\n",
    "                    # combine='nested',\n",
    "                    # concat_dim='time',\n",
    "                    parallel=True,\n",
    "                    engine='h5netcdf',\n",
    "                    chunks='auto'\n",
    "                )\n",
    "            elif dataset == \"ECCO_L4_DENS_STRAT_PRESS_05DEG_DAILY_V4R4\":\n",
    "                partial_func = partial(_preprocess_DENSITY, lon_bnds=lon_bnds, lat_bnds=lat_bnds)\n",
    "                ds = xr.open_mfdataset(\n",
    "                    files, \n",
    "                    preprocess=partial_func, \n",
    "                    join='outer', \n",
    "                    combine_attrs='drop_conflicts',\n",
    "                    # combine='nested',\n",
    "                    # concat_dim='time',\n",
    "                    parallel=True,\n",
    "                    engine='h5netcdf',\n",
    "                    chunks='auto'\n",
    "                )\n",
    "            elif dataset == \"AQUA_MODIS_L3M_DAILY_4KM_CHLCONC\" or dataset == \"AQUA_MODIS_L3M_DAILY_4KM_POC\" or dataset == \"AQUA_MODIS_L3M_DAILY_4KM_PIC\":\n",
    "                partial_func = partial(_preprocess_MODIS, lon_bnds=lon_bnds, lat_bnds=lat_bnds, dataset=dataset)\n",
    "                ds = xr.open_mfdataset(\n",
    "                    files, \n",
    "                    preprocess=partial_func, \n",
    "                    join='outer', \n",
    "                    combine_attrs='drop_conflicts',\n",
    "                    # combine='nested',\n",
    "                    # concat_dim='time',\n",
    "                    parallel=True,\n",
    "                    engine='h5netcdf',\n",
    "                    chunks='auto'\n",
    "                )\n",
    "            else:\n",
    "                partial_func = partial(_preprocess, lon_bnds=lon_bnds, lat_bnds=lat_bnds, dataset=dataset)\n",
    "                ds = xr.open_mfdataset(\n",
    "                    files, \n",
    "                    preprocess=partial_func, \n",
    "                    join='outer', \n",
    "                    combine_attrs='drop_conflicts',\n",
    "                    # combine='nested',\n",
    "                    # concat_dim='time',\n",
    "                    parallel=True,\n",
    "                    engine='h5netcdf',\n",
    "                    chunks='auto'\n",
    "                )\n",
    "\n",
    "            print(ds)\n",
    "            # ds.to_netcdf(new_dataset_path / (DATASETS_SHORTHAND[dataset] + \".nc\"), engine='h5netcdf')\n",
    "            ds.to_zarr(new_dataset_path / (DATASETS_SHORTHAND[dataset] + \".zarr\"), mode='w')\n",
    "            ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpaceApps2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
